import base64
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Any, Tuple

import cv2
import gradio as gr
import numpy as np
import torch
import torch.nn as nn
from pathlib import Path
import torchvision

# feat (Py-Feat) –¥–ª—è AU
try:
    from feat import Detector  # type: ignore
except Exception:
    Detector = None

# MediaPipe
try:
    import mediapipe as mp  # type: ignore
except Exception:
    mp = None

# ===== –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã =====

EMOTION_CLASSES: List[str] = [
    "angry",
    "disgust",
    "fear",
    "happy",
    "neutral",
    "sad",
    "surprise",
]

AGE_BIN_LABELS: List[str] = ["0-12", "13-25", "26-40", "41-60", "61+"]

AU_CLASSES = [
    "AU01",
    "AU02",
    "AU04",
    "AU05",
    "AU06",
    "AU07",
    "AU09",
    "AU10",
    "AU12",
    "AU14",
    "AU15",
    "AU17",
    "AU20",
    "AU23",
    "AU24",
    "AU25",
    "AU26",
]

AU_DESCRIPTIONS: Dict[str, str] = {
    "AU01": "–ø–æ–¥–Ω—è—Ç–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —á–∞—Å—Ç–∏ –±—Ä–æ–≤–µ–π",
    "AU02": "–ø–æ–¥–Ω—è—Ç–∏–µ –≤–Ω–µ—à–Ω–µ–π —á–∞—Å—Ç–∏ –±—Ä–æ–≤–µ–π",
    "AU04": "—Å–≤–µ–¥–µ–Ω–∏–µ –±—Ä–æ–≤–µ–π",
    "AU05": "–ø–æ–¥–Ω—è—Ç–∏–µ –≤–µ—Ä—Ö–Ω–µ–≥–æ –≤–µ–∫–∞",
    "AU06": "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ —â—ë–∫ (—É–ª—ã–±–∫–∞ –≥–ª–∞–∑–∞–º–∏)",
    "AU07": "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –Ω–∏–∂–Ω–µ–≥–æ –≤–µ–∫–∞",
    "AU09": "–º–æ—Ä—â–µ–Ω–∏–µ –Ω–æ—Å–∞",
    "AU10": "–ø–æ–¥–Ω—è—Ç–∏–µ –≤–µ—Ä—Ö–Ω–µ–π –≥—É–±—ã",
    "AU12": "—Ä–∞—Å—Ç—è–≥–∏–≤–∞–Ω–∏–µ —É–≥–æ–ª–∫–æ–≤ –≥—É–± (—É–ª—ã–±–∫–∞)",
    "AU14": "–∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è —É–ª—ã–±–∫–∞/—Å–∫–µ–ø—Å–∏—Å",
    "AU15": "–æ–ø—É—Å–∫–∞–Ω–∏–µ —É–≥–æ–ª–∫–æ–≤ –≥—É–±",
    "AU17": "–≤—ã–¥–≤–∏–∂–µ–Ω–∏–µ –ø–æ–¥–±–æ—Ä–æ–¥–∫–∞",
    "AU20": "—Ä–∞—Å—Ç—è–≥–∏–≤–∞–Ω–∏–µ –≥—É–± –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ",
    "AU23": "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –≥—É–±",
    "AU24": "—Å–∂–∞—Ç–∏–µ –≥—É–±",
    "AU25": "–æ—Ç–∫—Ä—ã—Ç–∏–µ —Ä—Ç–∞",
    "AU26": "—à–∏—Ä–æ–∫–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —Ä—Ç–∞",
}

_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# –ò—Å—Ç–æ—Ä–∏—è: (base64-–∫–∞—Ä—Ç–∏–Ω–∫–∞, –ø–æ–¥–ø–∏—Å—å)
history_entries: List[Tuple[str, str]] = []


# ===== –ú–û–î–ï–õ–ò =====

# --- Emotion (FER2013) ---
# –ù–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: ResNet18 (train_emotion_fer_resnet.py) -> models/emotion_resnet18.pth
# –§–æ–ª–ª–±–µ–∫: –ø—Ä–æ—Å—Ç–æ–π CNN (—Å—Ç–∞—Ä—ã–π) -> models/emotion_cnn.pth

FER_EMOTIONS = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]

def _strip_prefix_from_state_dict(state_dict: dict, prefix: str) -> dict:
    if not prefix:
        return state_dict
    out = {}
    for k, v in state_dict.items():
        if k.startswith(prefix):
            out[k[len(prefix):]] = v
        else:
            out[k] = v
    return out

def _infer_in_channels_from_resnet_state_dict(state_dict: dict) -> int:
    # resnet conv1 weight: [64, C, 7, 7]
    w = state_dict.get("conv1.weight")
    if w is None:
        # –∏–Ω–æ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∫–∞–∫ backbone.conv1.weight
        w = state_dict.get("backbone.conv1.weight")
    if w is None:
        return 3
    return int(w.shape[1])

def build_resnet18_for_classification(num_classes: int, in_channels: int = 3) -> nn.Module:
    model = torchvision.models.resnet18(weights=None)
    if in_channels != 3:
        # –∑–∞–º–µ–Ω–∏—Ç—å –ø–µ—Ä–≤—ã–π conv –ø–æ–¥ —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤
        model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model

class SimpleEmotionCNN(nn.Module):
    """–°—Ç–∞—Ä—ã–π (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π) –≤–∞—Ä–∏–∞–Ω—Ç. –û—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∫–∞–∫ fallback."""
    def __init__(self, num_classes: int = 7):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
        )
        self.classifier = nn.Linear(64, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


_emotion_model: Optional[nn.Module] = None
_emotion_backend: str = "none"   # "resnet18" | "simple_cnn" | "none"

# --- Age/Gender (UTKFace) ---
# –ù–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: ResNet18 multi-head (train_age_gender_utk_resnet.py) -> models/age_gender_resnet18.pth
# –§–æ–ª–ª–±–µ–∫: —Å—Ç–∞—Ä—ã–π CNN -> models/age_gender_cnn.pth

AGE_BIN_LABELS = ["0-12", "13-19", "20-29", "30-39", "40-49", "50-59", "60+"]

class AgeGenderCNN(nn.Module):
    def __init__(self, num_age_bins: int = 7):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
        )
        self.age_head = nn.Linear(64, num_age_bins)
        self.gender_head = nn.Linear(64, 2)  # 0=female, 1=male

    def forward(self, x: torch.Tensor):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        age_logits = self.age_head(x)
        gender_logits = self.gender_head(x)
        return age_logits, gender_logits


class AgeGenderResNet18(nn.Module):
    """ResNet18 + –¥–≤–µ –≥–æ–ª–æ–≤—ã: –≤–æ–∑—Ä–∞—Å—Ç (bin) –∏ –ø–æ–ª.
    –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —á–µ–∫–ø–æ–∏–Ω—Ç–∞: state_dict —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ (backbone.* / age_head.* / gender_head.*)
    –∏–ª–∏ state_dict –≥–æ–ª–æ–≥–æ resnet (conv1/layer*/fc) ‚Äî —Ç–æ–≥–¥–∞ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è, –Ω–æ –º–æ–¥–µ–ª—å –µ—â—ë –Ω–µ –æ–±—É—á–µ–Ω–∞,
    –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –º—è–≥–∫–æ –æ—Ç–∫–ª—é—á–∏–º –±–ª–æ–∫ –≤–æ–∑—Ä–∞—Å—Ç/–ø–æ–ª.
    """
    def __init__(self, num_age_bins: int = 7, in_channels: int = 3):
        super().__init__()
        self.backbone = torchvision.models.resnet18(weights=None)
        if in_channels != 3:
            self.backbone.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        feat_dim = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        self.age_head = nn.Linear(feat_dim, num_age_bins)
        self.gender_head = nn.Linear(feat_dim, 2)

    def forward(self, x: torch.Tensor):
        feats = self.backbone(x)
        age_logits = self.age_head(feats)
        gender_logits = self.gender_head(feats)
        return age_logits, gender_logits


_age_gender_model: Optional[nn.Module] = None
_age_gender_backend: str = "none"   # "resnet18" | "simple_cnn" | "none"


def load_models():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ ResNet18 –≤–µ—Ä—Å–∏–∏.
    –ï—Å–ª–∏ resnet-–≤–µ—Å–∞ –µ—â—ë –Ω–µ –≥–æ—Ç–æ–≤—ã/–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ—Å–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å),
    –∏–Ω–∞—á–µ —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã.
    """
    global _emotion_model, _emotion_backend, _age_gender_model, _age_gender_backend

    device = _device

    # --- EMOTION ---
    _emotion_model = None
    _emotion_backend = "none"

    # 1) emotion_resnet18.pth
    try:
        p = Path("models/emotion_resnet18.pth")
        if p.exists():
            sd = torch.load(p, map_location=device)
            if isinstance(sd, dict) and "state_dict" in sd:
                sd = sd["state_dict"]
            # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ backbone.*
            sd = _strip_prefix_from_state_dict(sd, "backbone.")
            in_ch = _infer_in_channels_from_resnet_state_dict(sd)
            m = build_resnet18_for_classification(num_classes=len(FER_EMOTIONS), in_channels=in_ch)
            missing, unexpected = m.load_state_dict(sd, strict=False)
            m.to(device).eval()
            _emotion_model = m
            _emotion_backend = "resnet18"
            print(f"[INFO] Emotion model: ResNet18 ({p}) | in_channels={in_ch} | missing={len(missing)} unexpected={len(unexpected)}")
    except Exception as e:
        print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å models/emotion_resnet18.pth: {e}")

    # 2) fallback emotion_cnn.pth
    if _emotion_model is None:
        try:
            p = Path("models/emotion_cnn.pth")
            if p.exists():
                m = SimpleEmotionCNN(num_classes=len(FER_EMOTIONS))
                sd = torch.load(p, map_location=device)
                if isinstance(sd, dict) and "state_dict" in sd:
                    sd = sd["state_dict"]
                m.load_state_dict(sd, strict=True)
                m.to(device).eval()
                _emotion_model = m
                _emotion_backend = "simple_cnn"
                print(f"[INFO] Emotion model: SimpleEmotionCNN ({p})")
        except Exception as e:
            print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å models/emotion_cnn.pth: {e}")

    # --- AGE/GENDER ---
    _age_gender_model = None
    _age_gender_backend = "none"

    # 1) age_gender_resnet18.pth (–º–æ–∂–µ—Ç –ø–æ—è–≤–∏—Ç—å—Å—è –ø–æ–∑–∂–µ)
    try:
        p = Path("models/age_gender_resnet18.pth")
        if p.exists():
            sd = torch.load(p, map_location=device)
            if isinstance(sd, dict) and "state_dict" in sd:
                sd = sd["state_dict"]
            # –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–Ω—è—Ç—å, —ç—Ç–æ state_dict –Ω–∞—à–µ–π multi-head –º–æ–¥–µ–ª–∏ –∏–ª–∏ —á–µ–≥–æ-—Ç–æ –¥—Ä—É–≥–æ–≥–æ
            # –µ—Å–ª–∏ –µ—Å—Ç—å backbone.* -> –Ω–∞—à–∞, –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å (–≤–¥—Ä—É–≥ —Å–æ–≤–ø–∞–ª–æ)
            in_ch = 3
            if "backbone.conv1.weight" in sd:
                in_ch = int(sd["backbone.conv1.weight"].shape[1])
            m = AgeGenderResNet18(num_age_bins=len(AGE_BIN_LABELS), in_channels=in_ch)
            missing, unexpected = m.load_state_dict(sd, strict=False)
            m.to(device).eval()
            _age_gender_model = m
            _age_gender_backend = "resnet18"
            print(f"[INFO] Age/Gender model: ResNet18-multihead ({p}) | missing={len(missing)} unexpected={len(unexpected)}")
    except Exception as e:
        print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å models/age_gender_resnet18.pth: {e}")

    # 2) fallback age_gender_cnn.pth
    if _age_gender_model is None:
        try:
            p = Path("models/age_gender_cnn.pth")
            if p.exists():
                m = AgeGenderCNN(num_age_bins=len(AGE_BIN_LABELS))
                sd = torch.load(p, map_location=device)
                if isinstance(sd, dict) and "state_dict" in sd:
                    sd = sd["state_dict"]
                m.load_state_dict(sd, strict=True)
                m.to(device).eval()
                _age_gender_model = m
                _age_gender_backend = "simple_cnn"
                print(f"[INFO] Age/Gender model: AgeGenderCNN ({p})")
        except Exception as e:
            print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å models/age_gender_cnn.pth: {e}")

    _use_trained_emotion_model = _emotion_model is not None
    if _emotion_model is None:
        print("[WARN] Emotion model: –ù–ï –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π).")
    _use_age_gender_model = _age_gender_model is not None
    if _age_gender_model is None:
        print("[WARN] Age/Gender model: –ù–ï –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–≤–æ–∑—Ä–∞—Å—Ç/–ø–æ–ª –±—É–¥—É—Ç —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π –∏–ª–∏ –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏).")


# –§–ª–∞–≥–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º
_use_trained_emotion_model = False
_use_age_gender_model = False

# ===== –î–µ—Ç–µ–∫—Ç–æ—Ä—ã –ª–∏—Ü =====

class OpenCVHaarFaceDetector:
    def __init__(self):
        self.detector = cv2.CascadeClassifier(
            cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
        )

    def detect(self, bgr_img) -> List[FaceBox]:
        gray = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)
        faces = self.detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)
        h, w = bgr_img.shape[:2]
        results: List[FaceBox] = []
        for (x, y, fw, fh) in faces:
            x1, y1 = max(0, int(x)), max(0, int(y))
            x2, y2 = min(w - 1, int(x + fw)), min(h - 1, int(y + fh))
            results.append(FaceBox(x1=x1, y1=y1, x2=x2, y2=y2, score=1.0))
        return results


class MediapipeFaceDetector:
    def __init__(self):
        if mp is None:
            raise RuntimeError("mediapipe –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        self.fd = mp.solutions.face_detection.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.5,
        )

    def detect(self, bgr_img) -> List[FaceBox]:
        h, w = bgr_img.shape[:2]
        rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
        res = self.fd.process(rgb)

        results: List[FaceBox] = []
        if not res.detections:
            return results

        img_area = h * w
        for det in res.detections:
            score = float(det.score[0]) if det.score else 0.0
            box = det.location_data.relative_bounding_box
            x1 = int(box.xmin * w)
            y1 = int(box.ymin * h)
            x2 = int((box.xmin + box.width) * w)
            y2 = int((box.ymin + box.height) * h)

            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w - 1, x2), min(h - 1, y2)
            if x2 <= x1 or y2 <= y1:
                continue

            area = (x2 - x1) * (y2 - y1)
            if score < 0.5 or area < 0.01 * img_area:
                continue

            results.append(FaceBox(x1=x1, y1=y1, x2=x2, y2=y2, score=score))

        return results


# ===== –≠–º–æ—Ü–∏–∏ =====

def dummy_emotion_predict(face_crop) -> Dict[str, float]:
    gray = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)
    m = float(np.mean(gray)) / 255.0
    probs = np.array(
        [0.05, 0.05, 0.05, 0.1 + 0.2 * m, 0.1, 0.05, 0.6 + 0.2 * (1 - m)],
        dtype=np.float32,
    )
    probs = np.clip(probs, 1e-3, 1.0)
    probs = probs / probs.sum()
    return {EMOTION_CLASSES[i]: float(probs[i]) for i in range(len(EMOTION_CLASSES))}


def emotion_predict_face_crop(face_crop_bgr) -> Optional[dict]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —ç–º–æ—Ü–∏–π –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –ª–∏—Ü–∞ (BGR).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 2 –±—ç–∫–µ–Ω–¥–∞:
      - ResNet18: models/emotion_resnet18.pth
      - fallback SimpleEmotionCNN: models/emotion_cnn.pth
    """
    if not _use_trained_emotion_model or _emotion_model is None:
        return None

    with torch.inference_mode():
        if _emotion_backend == "resnet18":
            # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ conv1
            in_ch = 3
            try:
                in_ch = int(getattr(_emotion_model, "conv1").in_channels)
            except Exception:
                pass

            if in_ch == 1:
                gray = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2GRAY)
                img = cv2.resize(gray, (224, 224))
                img = img.astype("float32") / 255.0
                img = np.expand_dims(img, axis=0)  # (1, H, W)
            else:
                rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)
                img = cv2.resize(rgb, (224, 224))
                img = img.astype("float32") / 255.0
                img = np.transpose(img, (2, 0, 1))  # (3, H, W)

            x = torch.tensor(img, dtype=torch.float32, device=_device).unsqueeze(0)  # (1,C,H,W)
            logits = _emotion_model(x)
            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()

        else:
            # —Å—Ç–∞—Ä—ã–π CNN (1 –∫–∞–Ω–∞–ª, 48x48)
            gray = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(gray, (48, 48))
            img = img.astype("float32") / 255.0
            img = np.expand_dims(img, axis=0)   # (1,H,W)
            img = np.expand_dims(img, axis=0)   # (1,1,H,W)
            tensor = torch.tensor(img, dtype=torch.float32, device=_device)
            logits = _emotion_model(tensor)
            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()

    probs_dict = {FER_EMOTIONS[i]: float(probs[i]) for i in range(len(FER_EMOTIONS))}
    return probs_dict

def age_gender_predict_face_crop(face_crop_bgr) -> (Optional[str], Optional[str]):
    """–í–æ–∑—Ä–∞—Å—Ç/–ø–æ–ª –ø–æ –ª–∏—Ü—É (BGR). –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –≤–µ—Ä–Ω—ë—Ç (None, None)."""
    if not _use_age_gender_model or _age_gender_model is None:
        return None, None

    with torch.inference_mode():
        if _age_gender_backend == "resnet18":
            # 224x224, 3 –∫–∞–Ω–∞–ª–∞
            rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)
            img = cv2.resize(rgb, (224, 224)).astype("float32") / 255.0
            img = np.transpose(img, (2, 0, 1))  # (3,H,W)
            x = torch.tensor(img, dtype=torch.float32, device=_device).unsqueeze(0)
            age_logits, gender_logits = _age_gender_model(x)
        else:
            # —Å—Ç–∞—Ä—ã–π CNN (64x64)
            rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)
            img = cv2.resize(rgb, (64, 64)).astype("float32") / 255.0
            img = np.transpose(img, (2, 0, 1))
            x = torch.tensor(img, dtype=torch.float32, device=_device).unsqueeze(0)
            age_logits, gender_logits = _age_gender_model(x)

        age_probs = torch.softmax(age_logits, dim=1)[0].cpu().numpy()
        gender_probs = torch.softmax(gender_logits, dim=1)[0].cpu().numpy()

    age_idx = int(np.argmax(age_probs))
    gender_idx = int(np.argmax(gender_probs))

    age_group = AGE_BIN_LABELS[age_idx] if 0 <= age_idx < len(AGE_BIN_LABELS) else None

    # gender_head: 0=female, 1=male
    conf = float(gender_probs[gender_idx])
    base_gender = "female" if gender_idx == 0 else "male"
    gender = f"{base_gender} (?)" if conf < 0.6 else base_gender

    return age_group, gender

def dummy_au_predict(face_crop) -> Dict[str, float]:
    h, w = face_crop.shape[:2]
    rng = np.random.default_rng(h * 1000 + w)
    probs = rng.random(len(AU_CLASSES)) * 0.6
    return {AU_CLASSES[i]: float(probs[i]) for i in range(len(AU_CLASSES))}


def au_predict_face_crop(face_crop_bgr) -> Dict[str, float]:
    if not _use_au_model or _au_detector is None:
        return dummy_au_predict(face_crop_bgr)

    rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)
    try:
        res = _au_detector.detect_image(rgb)
        if res.aus is None or res.aus.shape[0] == 0:
            return {}
        aus_series = res.aus.iloc[0]
        return {k: float(v) for k, v in aus_series.to_dict().items()}
    except Exception as e:
        print("[WARN] –û—à–∏–±–∫–∞ Py-Feat AU, fallback –Ω–∞ –∑–∞–≥–ª—É—à–∫—É:", e)
        return dummy_au_predict(face_crop_bgr)


# ===== –¢–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ (MediaPipe Pose) =====

def analyze_body(bgr_img) -> Optional[BodyResult]:
    if not _use_body_model or _pose_detector is None or mp_pose is None:
        return None

    h, w, _ = bgr_img.shape
    rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)

    try:
        res = _pose_detector.process(rgb)
    except Exception as e:
        print("[WARN] –û—à–∏–±–∫–∞ MediaPipe Pose:", e)
        return None

    if res.pose_landmarks is None:
        return None

    lms = res.pose_landmarks.landmark
    xs = [lm.x * w for lm in lms]
    ys = [lm.y * h for lm in lms]

    min_x, max_x = min(xs), max(xs)
    min_y, max_y = min(ys), max(ys)
    body_width = max_x - min_x
    body_height = max_y - min_y

    if body_height <= 0 or body_width <= 0:
        return None

    thickness_ratio = body_width / body_height  # —à–∏—Ä–∏–Ω–∞/—Ä–æ—Å—Ç

    try:
        ls = lms[mp_pose.PoseLandmark.LEFT_SHOULDER]
        rs = lms[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        lh = lms[mp_pose.PoseLandmark.LEFT_HIP]
        rh = lms[mp_pose.PoseLandmark.RIGHT_HIP]

        sx1, sy1 = ls.x * w, ls.y * h
        sx2, sy2 = rs.x * w, rs.y * h
        hx1, hy1 = lh.x * w, lh.y * h
        hx2, hy2 = rh.x * w, rh.y * h

        shoulder_width = float(np.hypot(sx2 - sx1, sy2 - sy1))
        hip_width = float(np.hypot(hx2 - hx1, hy2 - hy1))
        shoulder_hip_ratio = shoulder_width / hip_width if hip_width > 1e-3 else 1.0
    except Exception:
        shoulder_hip_ratio = 1.0

    if thickness_ratio < 0.25:
        slim, avg, large = 0.8, 0.15, 0.05
        base_cat = "—Ö—É–¥–æ—â–∞–≤–æ–µ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ"
    elif thickness_ratio < 0.35:
        slim, avg, large = 0.2, 0.7, 0.1
        base_cat = "–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ"
    else:
        slim, avg, large = 0.1, 0.2, 0.7
        base_cat = "–∫—Ä—É–ø–Ω–æ–µ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ"

    add = ""
    if shoulder_hip_ratio > 1.1:
        add = " (—à–∏—Ä–æ–∫–∏–µ –ø–ª–µ—á–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±—ë–¥–µ—Ä)"
    elif shoulder_hip_ratio < 0.9:
        add = " (—à–∏—Ä–æ–∫–∏–µ –±—ë–¥—Ä–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–ª–µ—á)"

    category = base_cat + add

    scores = {
        "slim": float(slim),
        "average": float(avg),
        "large": float(large),
        "thickness_ratio": float(thickness_ratio),
        "shoulder_hip_ratio": float(shoulder_hip_ratio),
    }

    return BodyResult(category=category, scores=scores)


def summarize_body(body: Optional[BodyResult]) -> str:
    if body is None:
        if _use_body_model:
            return "–¢–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ (–Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–¥—ë–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ–∑—É —á–µ–ª–æ–≤–µ–∫–∞)."
        else:
            return (
                "–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `mediapipe`, —á—Ç–æ–±—ã –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é."
            )

    parts = [f"**–¢–∏–ø —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏—è:** {body.category}."]
    slim = body.scores.get("slim")
    avg = body.scores.get("average")
    large = body.scores.get("large")

    if all(v is not None for v in [slim, avg, large]):
        parts.append(
            f"\n- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞): —Ö—É–¥–æ—â–∞–≤—ã–π ‚Äî {slim:.2f}, "
            f"–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π ‚Äî {avg:.2f}, –∫—Ä—É–ø–Ω—ã–π ‚Äî {large:.2f}."
        )

    thr = body.scores.get("thickness_ratio")
    shr = body.scores.get("shoulder_hip_ratio")
    if thr is not None and shr is not None:
        parts.append(
            f"\n- –û—Ç–Ω–æ—à–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã –∫ —Ä–æ—Å—Ç—É: {thr:.2f}; "
            f"–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã –ø–ª–µ—á –∫ –±—ë–¥—Ä–∞–º: {shr:.2f}."
        )

    return "".join(parts)


# ===== –ü–∞–π–ø–ª–∞–π–Ω =====

class AnalyzerPipeline:
    def __init__(self):
        try:
            self.face_detector = MediapipeFaceDetector()
            self.face_detector_name = "mediapipe_face_detection"
            print("[INFO] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü MediaPipe")
        except Exception as e:
            print("[WARN] MediaPipe FaceDetection –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, fallback –Ω–∞ Haar:", e)
            self.face_detector = OpenCVHaarFaceDetector()
            self.face_detector_name = "opencv_haar"

    def analyze(self, bgr_img: np.ndarray) -> AnalysisResult:
        used_fallback = False

        faces = self.face_detector.detect(bgr_img)
        if len(faces) == 0 and self.face_detector_name == "mediapipe_face_detection":
            haar = OpenCVHaarFaceDetector()
            faces = haar.detect(bgr_img)
            used_fallback = True

        face_results: List[FaceResult] = []

        for box in faces:
            crop = bgr_img[box.y1: box.y2, box.x1: box.x2]
            if crop.size == 0:
                continue

            emotion = emotion_predict_face_crop(crop)
            aus = au_predict_face_crop(crop)
            age_group, gender = age_gender_predict_face_crop(crop)

            face_results.append(
                FaceResult(
                    box=box,
                    emotion=emotion,
                    aus=aus,
                    age_group=age_group,
                    gender=gender,
                )
            )

        body_res = analyze_body(bgr_img)

        meta = {
            "num_faces": len(face_results),
            "image_shape": [int(bgr_img.shape[0]), int(bgr_img.shape[1])],
            "face_detector": getattr(self, "face_detector_name", "unknown"),
            "face_detector_fallback_to_haar": used_fallback,
            "use_trained_emotion_model": _use_trained_emotion_model,
            "use_age_gender_model": _use_age_gender_model,
            "use_au_model": _use_au_model,
            "use_body_model": _use_body_model,
            "note": (
                "–≠–º–æ—Ü–∏–∏: CNN (FER-2013) –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ models/emotion_cnn.pth. "
                "–í–æ–∑—Ä–∞—Å—Ç/–ø–æ–ª: CNN (UTKFace) –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ models/age_gender_cnn.pth. "
                "–ú–∏–º–∏–∫–∞: Py-Feat AU –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ feat. "
                "–¢–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ: —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ –ø–æ–∑–µ (MediaPipe Pose)."
            ),
        }

        return AnalysisResult(
            faces=face_results,
            body=body_res,
            meta=meta,
        )


pipeline = AnalyzerPipeline()


# ===== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====

def draw_faces(bgr_img, faces: List[FaceResult]):
    out = bgr_img.copy()
    for fr in faces:
        b = fr.box
        cv2.rectangle(out, (b.x1, b.y1), (b.x2, b.y2), (0, 255, 0), 2)

        top_em = (
            max(fr.emotion.items(), key=lambda x: x[1])[0]
            if fr.emotion
            else "face"
        )
        label = top_em
        if fr.gender is not None and fr.age_group is not None:
            label = f"{top_em} | {fr.gender}, {fr.age_group}"

        cv2.putText(
            out,
            label,
            (b.x1, max(0, b.y1 - 8)),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (0, 255, 0),
            2,
            cv2.LINE_AA,
        )
    return out


def encode_image_to_data_url(img_rgb: np.ndarray) -> str:
    thumb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    h, w = thumb.shape[:2]
    scale = 128 / max(h, w)
    if scale < 1.0:
        thumb = cv2.resize(
            thumb,
            (int(w * scale), int(h * scale)),
            interpolation=cv2.INTER_AREA,
        )
    ok, buf = cv2.imencode(".png", thumb)
    if not ok:
        return ""
    b64 = base64.b64encode(buf.tobytes()).decode("utf-8")
    return f"data:image/png;base64,{b64}"


def render_history_html(entries: List[Tuple[str, str]]) -> str:
    if not entries:
        return "<div class='history-empty'>–ò—Å—Ç–æ—Ä–∏—è –ø–æ–∫–∞ –ø—É—Å—Ç–∞.</div>"

    lines = ["<div class='history-list'>"]
    for data_url, caption in reversed(entries):
        lines.append(
            f"""
            <div class="history-item">
               <img src="{data_url}" alt="preview"/>
               <div class="history-caption">{caption}</div>
            </div>
            """
        )
    lines.append("</div>")
    return "\n".join(lines)


# ===== –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ =====

def analyze_image(img_rgb):
    global history_entries

    if img_rgb is None:
        return None, {}, [], "", render_history_html(history_entries)

    bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    result = pipeline.analyze(bgr)

    vis = draw_faces(bgr, result.faces)
    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)

    result_dict = {
        "faces": [
            {
                "box": asdict(fr.box),
                "emotion": fr.emotion,
                "aus": fr.aus,
                "age_group": fr.age_group,
                "gender": fr.gender,
            }
            for fr in result.faces
        ],
        "body": None
        if result.body is None
        else {
            "category": result.body.category,
            "scores": result.body.scores,
        },
        "meta": result.meta,
    }

    # –∫—Ä–∞—Ç–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞ –±–µ–∑ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π –ø—Ä–æ–∫—Ä—É—Ç–∫–∏:
    # face, emotion (—Ç–æ–ø-1), age_group, gender
    summary_rows: List[List[Any]] = []
    au_lines: List[str] = []

    for i, fr in enumerate(result.faces, start=1):
        if fr.emotion:
            top_em = max(fr.emotion.items(), key=lambda x: x[1])
            em_name, em_prob = top_em[0], top_em[1]
            em_str = f"{em_name} ({em_prob:.2f})"
        else:
            em_str = ""

        # —Ç–æ–ø-3 AU ‚Äî –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–º –±–ª–æ–∫–µ
        top3_au = sorted(
            (fr.aus or {}).items(), key=lambda x: x[1], reverse=True
        )[:3]
        if top3_au:
            au_desc_parts = []
            for k, v in top3_au:
                desc = AU_DESCRIPTIONS.get(k, "")
                if desc:
                    au_desc_parts.append(f"{k} ({desc}): {v:.2f}")
                else:
                    au_desc_parts.append(f"{k}: {v:.2f}")
            au_line = f"- –õ–∏—Ü–æ {i}: " + "; ".join(au_desc_parts)
            au_lines.append(au_line)

        summary_rows.append(
            [
                i,
                em_str,
                fr.age_group or "",
                fr.gender or "",
            ]
        )

    if not au_lines:
        au_text = "–ú–∏–º–∏–∫–∞ (AU): –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
    else:
        au_text = "#### –ú–∏–º–∏–∫–∞ (Action Units)\n" + "\n".join(au_lines)

    body_text = summarize_body(result.body)
    combined_md = au_text + "\n\n---\n\n#### –¢–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ\n" + body_text

    # –ò—Å—Ç–æ—Ä–∏—è
    if result.faces:
        main_face = result.faces[0]
        if main_face.emotion:
            main_em = max(main_face.emotion.items(), key=lambda x: x[1])[0]
        else:
            main_em = "unknown"
        ag = main_face.age_group or "?"
        gd = main_face.gender or "?"
        history_caption = f"{main_em}, {gd}, {ag}"
    else:
        history_caption = "–ª–∏—Ü–æ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"

    data_url = encode_image_to_data_url(img_rgb)
    if data_url:
        history_entries.append((data_url, history_caption))
        history_entries = history_entries[-5:]

    history_html = render_history_html(history_entries)

    return vis_rgb, result_dict, summary_rows, combined_md, history_html


def clear_form():
    """–û—á–∏—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π –≤–≤–æ–¥/–≤—ã–≤–æ–¥, –Ω–æ –∏—Å—Ç–æ—Ä–∏—é –ù–ï —Ç—Ä–æ–≥–∞–µ–º."""
    return None, None, {}, [], "", render_history_html(history_entries)


# ===== UI / –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ =====

custom_css = """
html, body {
    height: 100%;
    margin: 0;
    padding: 0;
    background: radial-gradient(circle at top left, #0f172a 0, #020617 45%, #020617 100%);
}

.gradio-container {
    max-width: 1200px !important;
    margin: 0 auto !important;
    background: transparent !important;
    color: #e5e7eb !important;
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
}

.gradio-app, .gradio-interface, .gr-block {
    background: transparent !important;
}

/* –í–µ—Ä—Ö–Ω–∏–π –±–∞—Ä */
#title-bar {
    background: linear-gradient(90deg, #1d4ed8, #22c55e);
    padding: 14px 20px;
    border-radius: 14px;
    color: white;
    margin: 16px 0 12px 0;
}
#title-bar h1 {
    font-size: 24px;
    margin: 0 0 4px 0;
}
#title-bar p {
    margin: 0;
    opacity: 0.9;
    font-size: 13px;
}

/* –ö–∞—Ä—Ç–æ—á–∫–∏ —Å–µ–∫—Ü–∏–π */
.section-card {
    background: rgba(15, 23, 42, 0.96) !important;
    border-radius: 16px;
    border: 1px solid rgba(148, 163, 184, 0.5);
    padding: 14px 16px !important;
    backdrop-filter: blur(12px);
}

/* –¢–µ–∫—Å—Ç */
.section-card .prose p,
.section-card .prose li,
.section-card .prose h1,
.section-card .prose h2,
.section-card .prose h3,
.section-card label,
.gradio-container .prose strong {
    color: #e5e7eb !important;
}

/* –ö–Ω–æ–ø–∫–∏ */
button {
    border-radius: 999px !important;
}

/* Dataframe —Ç—ë–º–Ω–∞—è */
.gr-dataframe table {
    background-color: #020617 !important;
    color: #e5e7eb !important;
    font-size: 13px !important;
}
.gr-dataframe th, .gr-dataframe td {
    background-color: #020617 !important;
    color: #e5e7eb !important;
    border-color: #1e293b !important;
}

/* –ò—Å—Ç–æ—Ä–∏—è */
.history-list {
    display: flex;
    flex-direction: column;
    gap: 8px;
}
.history-item {
    display: flex;
    align-items: center;
    gap: 10px;
    background: rgba(15, 23, 42, 0.9);
    border-radius: 12px;
    border: 1px solid rgba(148, 163, 184, 0.35);
    padding: 6px 8px;
}
.history-item img {
    width: 64px;
    height: 64px;
    border-radius: 10px;
    object-fit: cover;
}
.history-caption {
    font-size: 13px;
    color: #e5e7eb;
}
.history-empty {
    font-size: 13px;
    color: #9ca3af;
}
"""

with gr.Blocks(
    title="Visual Content Analyzer ‚Äî Demo MVP",
    css=custom_css,
    theme=gr.themes.Soft()
) as demo:

    with gr.Row():
        with gr.Column():
            gr.HTML(
                """
                <div id="title-bar">
                  <h1>Visual Content Analyzer</h1>
                  <p>–≠–º–æ—Ü–∏–∏ ‚Ä¢ –í–æ–∑—Ä–∞—Å—Ç –∏ –ø–æ–ª ‚Ä¢ –ú–∏–º–∏–∫–∞ (Action Units) ‚Ä¢ –¢–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ</p>
                </div>
                """
            )

    with gr.Row():
        with gr.Column(scale=5):
            with gr.Group(elem_classes="section-card"):
                gr.Markdown(
                    "### 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n"
                    "- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å –æ–¥–Ω–∏–º –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ª–∏—Ü–∞–º–∏\n"
                    "- –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –±—ã–ª –≤–∏–¥–µ–Ω –ø–æ –ø–æ—è—Å –∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é "
                    "–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏—è"
                )
                inp = gr.Image(type="numpy", label="–§–æ—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", height=420)

                with gr.Row():
                    analyze_btn = gr.Button("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å", variant="primary")
                    clear_btn = gr.Button("‚ôª –û—á–∏—Å—Ç–∏—Ç—å —Ñ–æ—Ä–º—É", variant="secondary")

            with gr.Group(elem_classes="section-card"):
                gr.Markdown(
                    "### –ü—Ä–∏–º–µ—Ä—ã\n"
                    "–ï—Å–ª–∏ –ø–æ–ª–æ–∂–∏—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ `assets/example_face1.jpg` –∏ "
                    "`assets/example_face2.jpg`, –∏—Ö –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥–µ–º–æ."
                )
                gr.Examples(
                    examples=[
                        ["assets/example_face1.jpg"],
                        ["assets/example_face2.jpg"],
                    ],
                    inputs=inp,
                    label="–î–µ–º–æ-—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏",
                )

        with gr.Column(scale=7):
            with gr.Group(elem_classes="section-card"):
                gr.Markdown("### 2Ô∏è‚É£ –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü –∏ —Å–≤–æ–¥–∫–∞ –ø–æ —á–µ–ª–æ–≤–µ–∫—É")
                out_img = gr.Image(
                    type="numpy", label="–†–µ–∑—É–ª—å—Ç–∞—Ç —Å –±–æ–∫—Å–∞–º–∏", height=420
                )

            with gr.Group(elem_classes="section-card"):
                with gr.Tabs():
                    with gr.Tab("–°–≤–æ–¥–∫–∞ –ø–æ –ª–∏—Ü–∞–º"):
                        out_table = gr.Dataframe(
                            headers=["face", "emotion", "age_group", "gender"],
                            datatype=["number", "str", "str", "str"],
                            label="–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø–æ –ª–∏—Ü–∞–º",
                        )
                    with gr.Tab("JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞"):
                        out_json = gr.JSON(label="–ü–æ–ª–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

                body_md = gr.Markdown(label="–ú–∏–º–∏–∫–∞ –∏ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ")

            with gr.Group(elem_classes="section-card"):
                gr.Markdown("### 3Ô∏è‚É£ –ò—Å—Ç–æ—Ä–∏—è –∞–Ω–∞–ª–∏–∑–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5)")
                history_html = gr.HTML(render_history_html(history_entries))

    gr.Markdown(
        "#### ‚ÑπÔ∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ\n"
        "- –≠–º–æ—Ü–∏–∏: –æ–±—É—á–µ–Ω–Ω–∞—è CNN –Ω–∞ FER-2013 –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ `models/emotion_cnn.pth`.\n"
        "- –í–æ–∑—Ä–∞—Å—Ç –∏ –ø–æ–ª: CNN –Ω–∞ UTKFace (aligned & cropped) –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ `models/age_gender_cnn.pth`.\n"
        "- –ú–∏–º–∏–∫–∞ (AU): Py-Feat (AU-–º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ BP4D/DISFA –∏ –¥—Ä.) –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `feat`.\n"
        "- –¢–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏–µ: –∞–Ω–∞–ª–∏–∑ –ø–æ–∑—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MediaPipe Pose –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ `mediapipe`.\n"
        "–ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∏ ‚Äî —ç—Ç–æ –≤–∏–¥–Ω–æ –≤ –ø–æ–ª–µ `meta` JSON."
    )

    analyze_btn.click(
        analyze_image,
        inputs=inp,
        outputs=[out_img, out_json, out_table, body_md, history_html],
    )

    clear_btn.click(
        clear_form,
        inputs=[],
        outputs=[inp, out_img, out_json, out_table, body_md, history_html],
    )


if __name__ == "__main__":
    demo.launch()